{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AishwaryaN2203/Deep_Learning/blob/main/Using_Optimizers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBehvqZ8znsy"
      },
      "source": [
        "# Using optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCdIqY0tKbvS"
      },
      "source": [
        "# Setting seeds to try and ensure we have the same results - this is not guaranteed across PyTorch releases.\n",
        "import torch\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(0)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCJzXv0OK1Bs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36fc8300-3809-49fe-af1f-0d2275b57a40"
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "mean, std = (0.5,), (0.5,)\n",
        "\n",
        "# Create a transform and normalise data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean, std)\n",
        "                              ])\n",
        "\n",
        "# Download FMNIST training dataset and load training data\n",
        "trainset = datasets.FashionMNIST('~/.pytorch/FMNIST/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Download FMNIST test dataset and load test data\n",
        "testset = datasets.FashionMNIST('~/.pytorch/FMNIST/', download=True, train=False, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /root/.pytorch/FMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:00<00:00, 102179560.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/FMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz to /root/.pytorch/FMNIST/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /root/.pytorch/FMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 5942534.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/FMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /root/.pytorch/FMNIST/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /root/.pytorch/FMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 4422102/4422102 [00:00<00:00, 62565196.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/FMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /root/.pytorch/FMNIST/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /root/.pytorch/FMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 23727776.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/FMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /root/.pytorch/FMNIST/FashionMNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqMqFbIVrbFH"
      },
      "source": [
        "class FMNIST(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(784, 128)\n",
        "    self.fc2 = nn.Linear(128,64)\n",
        "    self.fc3 = nn.Linear(64,10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.view(x.shape[0], -1)\n",
        "\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    x = F.log_softmax(x, dim=1)\n",
        "\n",
        "    return x\n",
        "\n",
        "#model = FMNIST()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c0QgxCF3fD-"
      },
      "source": [
        "model = nn.Sequential(nn.Linear(784, 128),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(128, 64),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(64, 10),\n",
        "                      nn.LogSoftmax(dim=1))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iPQek2nz2yu"
      },
      "source": [
        "images, labels = next(iter(trainloader))\n",
        "images = images.view(images.shape[0], -1)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roihp-kN0Jw5"
      },
      "source": [
        "from torch import optim\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "# lr - learning rate\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Nf2WdmP5Gst",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd576d71-ccf2-4ee4-b5ec-7397c2ffbe1a"
      },
      "source": [
        "output = model(images)\n",
        "loss = criterion(output, labels)\n",
        "loss.backward()\n",
        "print('Initial weights : ',model[0].weight)\n",
        "print('Initial weights gradient : ',model[0].weight.grad)\n",
        ""
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial weights :  Parameter containing:\n",
            "tensor([[-0.0003,  0.0192, -0.0294,  ...,  0.0219,  0.0037,  0.0021],\n",
            "        [-0.0198, -0.0150, -0.0104,  ..., -0.0203, -0.0060, -0.0299],\n",
            "        [-0.0201,  0.0149, -0.0333,  ..., -0.0203,  0.0012,  0.0080],\n",
            "        ...,\n",
            "        [ 0.0018, -0.0295,  0.0085,  ..., -0.0037,  0.0036,  0.0300],\n",
            "        [-0.0233, -0.0220, -0.0064,  ...,  0.0115, -0.0324, -0.0158],\n",
            "        [ 0.0309,  0.0066,  0.0125,  ...,  0.0286,  0.0350, -0.0105]],\n",
            "       requires_grad=True)\n",
            "Initial weights gradient :  tensor([[-0.0030, -0.0030, -0.0030,  ..., -0.0030, -0.0030, -0.0030],\n",
            "        [ 0.0022,  0.0022,  0.0022,  ...,  0.0024,  0.0022,  0.0022],\n",
            "        [ 0.0002,  0.0002,  0.0002,  ...,  0.0002,  0.0002,  0.0002],\n",
            "        ...,\n",
            "        [ 0.0014,  0.0014,  0.0014,  ...,  0.0014,  0.0014,  0.0014],\n",
            "        [ 0.0021,  0.0021,  0.0021,  ...,  0.0022,  0.0021,  0.0021],\n",
            "        [ 0.0038,  0.0038,  0.0038,  ...,  0.0038,  0.0038,  0.0038]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arwzAK-1EkEH"
      },
      "source": [
        "optimizer.step()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuGKi_nq6P0j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1507b59-c14c-4e66-857b-172852882c35"
      },
      "source": [
        "print('After running the optimizer \\n Initial weights : ',model[0].weight)\n",
        "print('Initial weights gradient : ',model[0].weight.grad)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After running the optimizer \n",
            " Initial weights :  Parameter containing:\n",
            "tensor([[-0.0002,  0.0192, -0.0294,  ...,  0.0220,  0.0038,  0.0021],\n",
            "        [-0.0198, -0.0150, -0.0105,  ..., -0.0203, -0.0060, -0.0300],\n",
            "        [-0.0202,  0.0149, -0.0333,  ..., -0.0203,  0.0012,  0.0080],\n",
            "        ...,\n",
            "        [ 0.0018, -0.0296,  0.0085,  ..., -0.0037,  0.0036,  0.0300],\n",
            "        [-0.0233, -0.0221, -0.0064,  ...,  0.0115, -0.0324, -0.0158],\n",
            "        [ 0.0309,  0.0065,  0.0125,  ...,  0.0285,  0.0349, -0.0106]],\n",
            "       requires_grad=True)\n",
            "Initial weights gradient :  tensor([[-0.0030, -0.0030, -0.0030,  ..., -0.0030, -0.0030, -0.0030],\n",
            "        [ 0.0022,  0.0022,  0.0022,  ...,  0.0024,  0.0022,  0.0022],\n",
            "        [ 0.0002,  0.0002,  0.0002,  ...,  0.0002,  0.0002,  0.0002],\n",
            "        ...,\n",
            "        [ 0.0014,  0.0014,  0.0014,  ...,  0.0014,  0.0014,  0.0014],\n",
            "        [ 0.0021,  0.0021,  0.0021,  ...,  0.0022,  0.0021,  0.0021],\n",
            "        [ 0.0038,  0.0038,  0.0038,  ...,  0.0038,  0.0038,  0.0038]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnfpzGigEpAr"
      },
      "source": [
        "optimizer.zero_grad()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EniqxHDwDa8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "229e61d1-4190-4bc0-f1a5-8887445b3524"
      },
      "source": [
        "print('Initial weights : ',model[0].weight)\n",
        "print('Initial weights gradient : ',model[0].weight.grad)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial weights :  Parameter containing:\n",
            "tensor([[-0.0002,  0.0192, -0.0294,  ...,  0.0220,  0.0038,  0.0021],\n",
            "        [-0.0198, -0.0150, -0.0105,  ..., -0.0203, -0.0060, -0.0300],\n",
            "        [-0.0202,  0.0149, -0.0333,  ..., -0.0203,  0.0012,  0.0080],\n",
            "        ...,\n",
            "        [ 0.0018, -0.0296,  0.0085,  ..., -0.0037,  0.0036,  0.0300],\n",
            "        [-0.0233, -0.0221, -0.0064,  ...,  0.0115, -0.0324, -0.0158],\n",
            "        [ 0.0309,  0.0065,  0.0125,  ...,  0.0285,  0.0349, -0.0106]],\n",
            "       requires_grad=True)\n",
            "Initial weights gradient :  None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DViAViGEwyr"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGZhQE3tDcqb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b21acefa-8dcc-42d0-ecab-c3b42ffa4e96"
      },
      "source": [
        "model = FMNIST()\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "num_epochs = 1\n",
        "\n",
        "for i in range(num_epochs):\n",
        "    cum_loss = 0\n",
        "    batch_num = 0\n",
        "\n",
        "    for batch_num, (images, labels) in enumerate(trainloader, 1):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(images)\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        cum_loss += loss.item()\n",
        "\n",
        "        print(f\"Batch Num:{batch_num}, Loss: {loss.item()}\")\n",
        "\n",
        "\n",
        "    print(f\"Training loss: {cum_loss/len(trainloader)}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Num:1, Loss: 2.2870397567749023\n",
            "Batch Num:2, Loss: 2.304734468460083\n",
            "Batch Num:3, Loss: 2.28812313079834\n",
            "Batch Num:4, Loss: 2.2898643016815186\n",
            "Batch Num:5, Loss: 2.2802863121032715\n",
            "Batch Num:6, Loss: 2.2768282890319824\n",
            "Batch Num:7, Loss: 2.2971038818359375\n",
            "Batch Num:8, Loss: 2.272695779800415\n",
            "Batch Num:9, Loss: 2.266519546508789\n",
            "Batch Num:10, Loss: 2.276360273361206\n",
            "Batch Num:11, Loss: 2.285900115966797\n",
            "Batch Num:12, Loss: 2.2687723636627197\n",
            "Batch Num:13, Loss: 2.2825448513031006\n",
            "Batch Num:14, Loss: 2.269296407699585\n",
            "Batch Num:15, Loss: 2.251699447631836\n",
            "Batch Num:16, Loss: 2.2710673809051514\n",
            "Batch Num:17, Loss: 2.2565464973449707\n",
            "Batch Num:18, Loss: 2.247075080871582\n",
            "Batch Num:19, Loss: 2.242631673812866\n",
            "Batch Num:20, Loss: 2.2493398189544678\n",
            "Batch Num:21, Loss: 2.2490806579589844\n",
            "Batch Num:22, Loss: 2.239584445953369\n",
            "Batch Num:23, Loss: 2.2394418716430664\n",
            "Batch Num:24, Loss: 2.252643585205078\n",
            "Batch Num:25, Loss: 2.2172629833221436\n",
            "Batch Num:26, Loss: 2.236956834793091\n",
            "Batch Num:27, Loss: 2.2230703830718994\n",
            "Batch Num:28, Loss: 2.223057270050049\n",
            "Batch Num:29, Loss: 2.197035074234009\n",
            "Batch Num:30, Loss: 2.2178027629852295\n",
            "Batch Num:31, Loss: 2.242530107498169\n",
            "Batch Num:32, Loss: 2.223618507385254\n",
            "Batch Num:33, Loss: 2.182101011276245\n",
            "Batch Num:34, Loss: 2.1979777812957764\n",
            "Batch Num:35, Loss: 2.219874382019043\n",
            "Batch Num:36, Loss: 2.2039496898651123\n",
            "Batch Num:37, Loss: 2.219465970993042\n",
            "Batch Num:38, Loss: 2.1876096725463867\n",
            "Batch Num:39, Loss: 2.1937692165374756\n",
            "Batch Num:40, Loss: 2.199791193008423\n",
            "Batch Num:41, Loss: 2.1997416019439697\n",
            "Batch Num:42, Loss: 2.199578046798706\n",
            "Batch Num:43, Loss: 2.166189670562744\n",
            "Batch Num:44, Loss: 2.1731696128845215\n",
            "Batch Num:45, Loss: 2.1663475036621094\n",
            "Batch Num:46, Loss: 2.1630232334136963\n",
            "Batch Num:47, Loss: 2.173583745956421\n",
            "Batch Num:48, Loss: 2.166243314743042\n",
            "Batch Num:49, Loss: 2.17478609085083\n",
            "Batch Num:50, Loss: 2.1410579681396484\n",
            "Batch Num:51, Loss: 2.132265329360962\n",
            "Batch Num:52, Loss: 2.1887452602386475\n",
            "Batch Num:53, Loss: 2.162026882171631\n",
            "Batch Num:54, Loss: 2.117699146270752\n",
            "Batch Num:55, Loss: 2.141117572784424\n",
            "Batch Num:56, Loss: 2.135773181915283\n",
            "Batch Num:57, Loss: 2.1412599086761475\n",
            "Batch Num:58, Loss: 2.1211438179016113\n",
            "Batch Num:59, Loss: 2.103712320327759\n",
            "Batch Num:60, Loss: 2.105754852294922\n",
            "Batch Num:61, Loss: 2.1260483264923096\n",
            "Batch Num:62, Loss: 2.0871195793151855\n",
            "Batch Num:63, Loss: 2.0897393226623535\n",
            "Batch Num:64, Loss: 2.1417412757873535\n",
            "Batch Num:65, Loss: 2.0826640129089355\n",
            "Batch Num:66, Loss: 2.086456298828125\n",
            "Batch Num:67, Loss: 2.0807738304138184\n",
            "Batch Num:68, Loss: 2.0981459617614746\n",
            "Batch Num:69, Loss: 2.084650993347168\n",
            "Batch Num:70, Loss: 2.0585594177246094\n",
            "Batch Num:71, Loss: 2.083326816558838\n",
            "Batch Num:72, Loss: 2.0672619342803955\n",
            "Batch Num:73, Loss: 2.045077085494995\n",
            "Batch Num:74, Loss: 2.0248615741729736\n",
            "Batch Num:75, Loss: 2.0355846881866455\n",
            "Batch Num:76, Loss: 2.0456814765930176\n",
            "Batch Num:77, Loss: 2.042203426361084\n",
            "Batch Num:78, Loss: 2.0114800930023193\n",
            "Batch Num:79, Loss: 2.0007989406585693\n",
            "Batch Num:80, Loss: 2.0284690856933594\n",
            "Batch Num:81, Loss: 2.050344228744507\n",
            "Batch Num:82, Loss: 1.9999029636383057\n",
            "Batch Num:83, Loss: 2.040477752685547\n",
            "Batch Num:84, Loss: 2.004279613494873\n",
            "Batch Num:85, Loss: 2.0411429405212402\n",
            "Batch Num:86, Loss: 1.9856860637664795\n",
            "Batch Num:87, Loss: 1.984741449356079\n",
            "Batch Num:88, Loss: 1.9755622148513794\n",
            "Batch Num:89, Loss: 1.9781358242034912\n",
            "Batch Num:90, Loss: 1.971121907234192\n",
            "Batch Num:91, Loss: 2.0088024139404297\n",
            "Batch Num:92, Loss: 1.9533547163009644\n",
            "Batch Num:93, Loss: 1.9273675680160522\n",
            "Batch Num:94, Loss: 1.9663947820663452\n",
            "Batch Num:95, Loss: 1.9294841289520264\n",
            "Batch Num:96, Loss: 1.8992468118667603\n",
            "Batch Num:97, Loss: 1.913975715637207\n",
            "Batch Num:98, Loss: 1.920300841331482\n",
            "Batch Num:99, Loss: 1.893148422241211\n",
            "Batch Num:100, Loss: 1.9020233154296875\n",
            "Batch Num:101, Loss: 1.9861621856689453\n",
            "Batch Num:102, Loss: 1.8941391706466675\n",
            "Batch Num:103, Loss: 1.878528118133545\n",
            "Batch Num:104, Loss: 1.8526651859283447\n",
            "Batch Num:105, Loss: 1.8771803379058838\n",
            "Batch Num:106, Loss: 1.8768731355667114\n",
            "Batch Num:107, Loss: 1.8324782848358154\n",
            "Batch Num:108, Loss: 1.8375011682510376\n",
            "Batch Num:109, Loss: 1.8727120161056519\n",
            "Batch Num:110, Loss: 1.8134828805923462\n",
            "Batch Num:111, Loss: 1.8504548072814941\n",
            "Batch Num:112, Loss: 1.8896410465240479\n",
            "Batch Num:113, Loss: 1.8046207427978516\n",
            "Batch Num:114, Loss: 1.8042619228363037\n",
            "Batch Num:115, Loss: 1.7859387397766113\n",
            "Batch Num:116, Loss: 1.7643966674804688\n",
            "Batch Num:117, Loss: 1.7738773822784424\n",
            "Batch Num:118, Loss: 1.814422607421875\n",
            "Batch Num:119, Loss: 1.778023600578308\n",
            "Batch Num:120, Loss: 1.7875747680664062\n",
            "Batch Num:121, Loss: 1.7815160751342773\n",
            "Batch Num:122, Loss: 1.7425153255462646\n",
            "Batch Num:123, Loss: 1.6732897758483887\n",
            "Batch Num:124, Loss: 1.8245290517807007\n",
            "Batch Num:125, Loss: 1.7087002992630005\n",
            "Batch Num:126, Loss: 1.7019672393798828\n",
            "Batch Num:127, Loss: 1.721555471420288\n",
            "Batch Num:128, Loss: 1.6888344287872314\n",
            "Batch Num:129, Loss: 1.735489845275879\n",
            "Batch Num:130, Loss: 1.6645172834396362\n",
            "Batch Num:131, Loss: 1.6880850791931152\n",
            "Batch Num:132, Loss: 1.6604924201965332\n",
            "Batch Num:133, Loss: 1.6661121845245361\n",
            "Batch Num:134, Loss: 1.6280564069747925\n",
            "Batch Num:135, Loss: 1.6878329515457153\n",
            "Batch Num:136, Loss: 1.6056441068649292\n",
            "Batch Num:137, Loss: 1.6899101734161377\n",
            "Batch Num:138, Loss: 1.5744436979293823\n",
            "Batch Num:139, Loss: 1.6058402061462402\n",
            "Batch Num:140, Loss: 1.6007808446884155\n",
            "Batch Num:141, Loss: 1.5656627416610718\n",
            "Batch Num:142, Loss: 1.630824327468872\n",
            "Batch Num:143, Loss: 1.6335316896438599\n",
            "Batch Num:144, Loss: 1.5379682779312134\n",
            "Batch Num:145, Loss: 1.589952826499939\n",
            "Batch Num:146, Loss: 1.5329880714416504\n",
            "Batch Num:147, Loss: 1.5864728689193726\n",
            "Batch Num:148, Loss: 1.5517464876174927\n",
            "Batch Num:149, Loss: 1.681037425994873\n",
            "Batch Num:150, Loss: 1.5942589044570923\n",
            "Batch Num:151, Loss: 1.5094536542892456\n",
            "Batch Num:152, Loss: 1.5328160524368286\n",
            "Batch Num:153, Loss: 1.5457831621170044\n",
            "Batch Num:154, Loss: 1.4847909212112427\n",
            "Batch Num:155, Loss: 1.5357011556625366\n",
            "Batch Num:156, Loss: 1.4713897705078125\n",
            "Batch Num:157, Loss: 1.547956943511963\n",
            "Batch Num:158, Loss: 1.447540283203125\n",
            "Batch Num:159, Loss: 1.4744549989700317\n",
            "Batch Num:160, Loss: 1.511656641960144\n",
            "Batch Num:161, Loss: 1.4856829643249512\n",
            "Batch Num:162, Loss: 1.391874074935913\n",
            "Batch Num:163, Loss: 1.4790599346160889\n",
            "Batch Num:164, Loss: 1.4736872911453247\n",
            "Batch Num:165, Loss: 1.4298242330551147\n",
            "Batch Num:166, Loss: 1.41767418384552\n",
            "Batch Num:167, Loss: 1.573803186416626\n",
            "Batch Num:168, Loss: 1.4504848718643188\n",
            "Batch Num:169, Loss: 1.4643325805664062\n",
            "Batch Num:170, Loss: 1.4500443935394287\n",
            "Batch Num:171, Loss: 1.3603771924972534\n",
            "Batch Num:172, Loss: 1.4941645860671997\n",
            "Batch Num:173, Loss: 1.3527286052703857\n",
            "Batch Num:174, Loss: 1.4247959852218628\n",
            "Batch Num:175, Loss: 1.4404929876327515\n",
            "Batch Num:176, Loss: 1.3351831436157227\n",
            "Batch Num:177, Loss: 1.4884059429168701\n",
            "Batch Num:178, Loss: 1.448508858680725\n",
            "Batch Num:179, Loss: 1.4672386646270752\n",
            "Batch Num:180, Loss: 1.4081943035125732\n",
            "Batch Num:181, Loss: 1.3293312788009644\n",
            "Batch Num:182, Loss: 1.3573660850524902\n",
            "Batch Num:183, Loss: 1.3603969812393188\n",
            "Batch Num:184, Loss: 1.404849886894226\n",
            "Batch Num:185, Loss: 1.3898038864135742\n",
            "Batch Num:186, Loss: 1.3744630813598633\n",
            "Batch Num:187, Loss: 1.4307758808135986\n",
            "Batch Num:188, Loss: 1.3570908308029175\n",
            "Batch Num:189, Loss: 1.2827563285827637\n",
            "Batch Num:190, Loss: 1.2756606340408325\n",
            "Batch Num:191, Loss: 1.2754038572311401\n",
            "Batch Num:192, Loss: 1.27769935131073\n",
            "Batch Num:193, Loss: 1.211912751197815\n",
            "Batch Num:194, Loss: 1.2995885610580444\n",
            "Batch Num:195, Loss: 1.3091521263122559\n",
            "Batch Num:196, Loss: 1.449568748474121\n",
            "Batch Num:197, Loss: 1.2857229709625244\n",
            "Batch Num:198, Loss: 1.3289313316345215\n",
            "Batch Num:199, Loss: 1.2628916501998901\n",
            "Batch Num:200, Loss: 1.241918921470642\n",
            "Batch Num:201, Loss: 1.248958945274353\n",
            "Batch Num:202, Loss: 1.3038017749786377\n",
            "Batch Num:203, Loss: 1.2043706178665161\n",
            "Batch Num:204, Loss: 1.2874937057495117\n",
            "Batch Num:205, Loss: 1.2431583404541016\n",
            "Batch Num:206, Loss: 1.1754915714263916\n",
            "Batch Num:207, Loss: 1.2307859659194946\n",
            "Batch Num:208, Loss: 1.1404945850372314\n",
            "Batch Num:209, Loss: 1.36781644821167\n",
            "Batch Num:210, Loss: 1.2619426250457764\n",
            "Batch Num:211, Loss: 1.1684428453445435\n",
            "Batch Num:212, Loss: 1.3198182582855225\n",
            "Batch Num:213, Loss: 1.0796617269515991\n",
            "Batch Num:214, Loss: 1.117333173751831\n",
            "Batch Num:215, Loss: 1.2286982536315918\n",
            "Batch Num:216, Loss: 1.1974061727523804\n",
            "Batch Num:217, Loss: 1.2083951234817505\n",
            "Batch Num:218, Loss: 1.2510026693344116\n",
            "Batch Num:219, Loss: 1.0543642044067383\n",
            "Batch Num:220, Loss: 1.2688978910446167\n",
            "Batch Num:221, Loss: 1.27896249294281\n",
            "Batch Num:222, Loss: 1.2129831314086914\n",
            "Batch Num:223, Loss: 1.1825497150421143\n",
            "Batch Num:224, Loss: 1.1791727542877197\n",
            "Batch Num:225, Loss: 1.3311452865600586\n",
            "Batch Num:226, Loss: 1.1154332160949707\n",
            "Batch Num:227, Loss: 1.1277801990509033\n",
            "Batch Num:228, Loss: 1.0481840372085571\n",
            "Batch Num:229, Loss: 1.1764765977859497\n",
            "Batch Num:230, Loss: 1.2211635112762451\n",
            "Batch Num:231, Loss: 1.30003023147583\n",
            "Batch Num:232, Loss: 1.150438904762268\n",
            "Batch Num:233, Loss: 1.1533278226852417\n",
            "Batch Num:234, Loss: 1.1004990339279175\n",
            "Batch Num:235, Loss: 1.13161301612854\n",
            "Batch Num:236, Loss: 1.092271327972412\n",
            "Batch Num:237, Loss: 1.244462013244629\n",
            "Batch Num:238, Loss: 1.098931074142456\n",
            "Batch Num:239, Loss: 1.1379446983337402\n",
            "Batch Num:240, Loss: 1.128068447113037\n",
            "Batch Num:241, Loss: 1.203403353691101\n",
            "Batch Num:242, Loss: 1.1087865829467773\n",
            "Batch Num:243, Loss: 1.1837549209594727\n",
            "Batch Num:244, Loss: 1.174514889717102\n",
            "Batch Num:245, Loss: 1.1264530420303345\n",
            "Batch Num:246, Loss: 1.1874589920043945\n",
            "Batch Num:247, Loss: 1.141237735748291\n",
            "Batch Num:248, Loss: 1.0862826108932495\n",
            "Batch Num:249, Loss: 0.9838451743125916\n",
            "Batch Num:250, Loss: 1.0992698669433594\n",
            "Batch Num:251, Loss: 1.0259323120117188\n",
            "Batch Num:252, Loss: 1.0281435251235962\n",
            "Batch Num:253, Loss: 1.0866336822509766\n",
            "Batch Num:254, Loss: 1.0587654113769531\n",
            "Batch Num:255, Loss: 1.0446803569793701\n",
            "Batch Num:256, Loss: 1.164698600769043\n",
            "Batch Num:257, Loss: 1.0904698371887207\n",
            "Batch Num:258, Loss: 0.9964820742607117\n",
            "Batch Num:259, Loss: 1.0643682479858398\n",
            "Batch Num:260, Loss: 1.076379418373108\n",
            "Batch Num:261, Loss: 1.0360397100448608\n",
            "Batch Num:262, Loss: 1.0996217727661133\n",
            "Batch Num:263, Loss: 0.9848844408988953\n",
            "Batch Num:264, Loss: 1.0447592735290527\n",
            "Batch Num:265, Loss: 1.08978271484375\n",
            "Batch Num:266, Loss: 1.0932958126068115\n",
            "Batch Num:267, Loss: 1.1169300079345703\n",
            "Batch Num:268, Loss: 1.1510318517684937\n",
            "Batch Num:269, Loss: 1.0899593830108643\n",
            "Batch Num:270, Loss: 1.1003568172454834\n",
            "Batch Num:271, Loss: 1.071834921836853\n",
            "Batch Num:272, Loss: 1.0314911603927612\n",
            "Batch Num:273, Loss: 1.0488630533218384\n",
            "Batch Num:274, Loss: 1.1296406984329224\n",
            "Batch Num:275, Loss: 1.0038394927978516\n",
            "Batch Num:276, Loss: 1.1300654411315918\n",
            "Batch Num:277, Loss: 1.0546897649765015\n",
            "Batch Num:278, Loss: 1.11454439163208\n",
            "Batch Num:279, Loss: 1.0150665044784546\n",
            "Batch Num:280, Loss: 1.0460902452468872\n",
            "Batch Num:281, Loss: 1.1339049339294434\n",
            "Batch Num:282, Loss: 0.9761180877685547\n",
            "Batch Num:283, Loss: 1.0654933452606201\n",
            "Batch Num:284, Loss: 0.920923113822937\n",
            "Batch Num:285, Loss: 1.080431580543518\n",
            "Batch Num:286, Loss: 0.9702100157737732\n",
            "Batch Num:287, Loss: 0.9916756749153137\n",
            "Batch Num:288, Loss: 0.9429172277450562\n",
            "Batch Num:289, Loss: 1.0510786771774292\n",
            "Batch Num:290, Loss: 0.9330554008483887\n",
            "Batch Num:291, Loss: 1.0219699144363403\n",
            "Batch Num:292, Loss: 1.046406626701355\n",
            "Batch Num:293, Loss: 1.0180610418319702\n",
            "Batch Num:294, Loss: 1.0389397144317627\n",
            "Batch Num:295, Loss: 1.038696527481079\n",
            "Batch Num:296, Loss: 1.010002613067627\n",
            "Batch Num:297, Loss: 0.9653136134147644\n",
            "Batch Num:298, Loss: 0.9908828139305115\n",
            "Batch Num:299, Loss: 1.0952585935592651\n",
            "Batch Num:300, Loss: 0.9550024271011353\n",
            "Batch Num:301, Loss: 0.8943036198616028\n",
            "Batch Num:302, Loss: 1.0015145540237427\n",
            "Batch Num:303, Loss: 1.0458190441131592\n",
            "Batch Num:304, Loss: 0.9904091358184814\n",
            "Batch Num:305, Loss: 0.9286434054374695\n",
            "Batch Num:306, Loss: 0.893896758556366\n",
            "Batch Num:307, Loss: 1.0463982820510864\n",
            "Batch Num:308, Loss: 0.9978644847869873\n",
            "Batch Num:309, Loss: 0.9947817325592041\n",
            "Batch Num:310, Loss: 1.0235381126403809\n",
            "Batch Num:311, Loss: 0.8400946259498596\n",
            "Batch Num:312, Loss: 0.9030163884162903\n",
            "Batch Num:313, Loss: 1.0359846353530884\n",
            "Batch Num:314, Loss: 0.8487733602523804\n",
            "Batch Num:315, Loss: 1.0310972929000854\n",
            "Batch Num:316, Loss: 0.8895847201347351\n",
            "Batch Num:317, Loss: 0.9951984286308289\n",
            "Batch Num:318, Loss: 0.9706330895423889\n",
            "Batch Num:319, Loss: 0.8424293398857117\n",
            "Batch Num:320, Loss: 1.0455058813095093\n",
            "Batch Num:321, Loss: 0.9383759498596191\n",
            "Batch Num:322, Loss: 0.9427353739738464\n",
            "Batch Num:323, Loss: 1.0756959915161133\n",
            "Batch Num:324, Loss: 0.8923153877258301\n",
            "Batch Num:325, Loss: 0.8994468450546265\n",
            "Batch Num:326, Loss: 0.8154083490371704\n",
            "Batch Num:327, Loss: 0.8237285614013672\n",
            "Batch Num:328, Loss: 0.9863319993019104\n",
            "Batch Num:329, Loss: 0.8959537148475647\n",
            "Batch Num:330, Loss: 0.8395195007324219\n",
            "Batch Num:331, Loss: 0.8635371923446655\n",
            "Batch Num:332, Loss: 0.9945718050003052\n",
            "Batch Num:333, Loss: 0.841984748840332\n",
            "Batch Num:334, Loss: 0.9896259307861328\n",
            "Batch Num:335, Loss: 0.9663033485412598\n",
            "Batch Num:336, Loss: 0.9501191973686218\n",
            "Batch Num:337, Loss: 0.9832263588905334\n",
            "Batch Num:338, Loss: 0.87591153383255\n",
            "Batch Num:339, Loss: 0.8855512738227844\n",
            "Batch Num:340, Loss: 0.9511435627937317\n",
            "Batch Num:341, Loss: 0.9434107542037964\n",
            "Batch Num:342, Loss: 0.7490012645721436\n",
            "Batch Num:343, Loss: 0.8747326731681824\n",
            "Batch Num:344, Loss: 1.0233352184295654\n",
            "Batch Num:345, Loss: 0.8393663763999939\n",
            "Batch Num:346, Loss: 0.8569079041481018\n",
            "Batch Num:347, Loss: 0.8220751881599426\n",
            "Batch Num:348, Loss: 0.8316240906715393\n",
            "Batch Num:349, Loss: 0.9496402144432068\n",
            "Batch Num:350, Loss: 0.9732807874679565\n",
            "Batch Num:351, Loss: 0.8508480191230774\n",
            "Batch Num:352, Loss: 0.8344924449920654\n",
            "Batch Num:353, Loss: 1.0469400882720947\n",
            "Batch Num:354, Loss: 0.9938141703605652\n",
            "Batch Num:355, Loss: 0.8272982835769653\n",
            "Batch Num:356, Loss: 0.822719931602478\n",
            "Batch Num:357, Loss: 0.9246723055839539\n",
            "Batch Num:358, Loss: 0.8255587220191956\n",
            "Batch Num:359, Loss: 0.921745777130127\n",
            "Batch Num:360, Loss: 1.1145857572555542\n",
            "Batch Num:361, Loss: 0.9136335849761963\n",
            "Batch Num:362, Loss: 1.0561153888702393\n",
            "Batch Num:363, Loss: 0.8876591920852661\n",
            "Batch Num:364, Loss: 0.964198648929596\n",
            "Batch Num:365, Loss: 1.0136845111846924\n",
            "Batch Num:366, Loss: 0.911388635635376\n",
            "Batch Num:367, Loss: 0.9671041965484619\n",
            "Batch Num:368, Loss: 0.8182908296585083\n",
            "Batch Num:369, Loss: 0.698824405670166\n",
            "Batch Num:370, Loss: 0.811265766620636\n",
            "Batch Num:371, Loss: 1.02518892288208\n",
            "Batch Num:372, Loss: 0.9694781303405762\n",
            "Batch Num:373, Loss: 0.7632171511650085\n",
            "Batch Num:374, Loss: 0.911081075668335\n",
            "Batch Num:375, Loss: 0.8206045627593994\n",
            "Batch Num:376, Loss: 0.8551994562149048\n",
            "Batch Num:377, Loss: 0.786819338798523\n",
            "Batch Num:378, Loss: 1.02726411819458\n",
            "Batch Num:379, Loss: 0.8709172606468201\n",
            "Batch Num:380, Loss: 0.974528968334198\n",
            "Batch Num:381, Loss: 0.864437997341156\n",
            "Batch Num:382, Loss: 0.7654017806053162\n",
            "Batch Num:383, Loss: 0.8527773022651672\n",
            "Batch Num:384, Loss: 0.951980710029602\n",
            "Batch Num:385, Loss: 0.8120833039283752\n",
            "Batch Num:386, Loss: 0.989862859249115\n",
            "Batch Num:387, Loss: 0.957482635974884\n",
            "Batch Num:388, Loss: 0.8776814341545105\n",
            "Batch Num:389, Loss: 0.7408664226531982\n",
            "Batch Num:390, Loss: 0.857637882232666\n",
            "Batch Num:391, Loss: 0.8054623603820801\n",
            "Batch Num:392, Loss: 0.8060972690582275\n",
            "Batch Num:393, Loss: 0.8291682600975037\n",
            "Batch Num:394, Loss: 0.9052843451499939\n",
            "Batch Num:395, Loss: 0.860846221446991\n",
            "Batch Num:396, Loss: 0.7162651419639587\n",
            "Batch Num:397, Loss: 0.7144876718521118\n",
            "Batch Num:398, Loss: 0.777045726776123\n",
            "Batch Num:399, Loss: 0.776336133480072\n",
            "Batch Num:400, Loss: 0.7847138047218323\n",
            "Batch Num:401, Loss: 0.7018038034439087\n",
            "Batch Num:402, Loss: 0.7162075042724609\n",
            "Batch Num:403, Loss: 0.8036730289459229\n",
            "Batch Num:404, Loss: 0.869994580745697\n",
            "Batch Num:405, Loss: 0.7052884697914124\n",
            "Batch Num:406, Loss: 0.8673902750015259\n",
            "Batch Num:407, Loss: 0.9441889524459839\n",
            "Batch Num:408, Loss: 0.7974622249603271\n",
            "Batch Num:409, Loss: 0.759787917137146\n",
            "Batch Num:410, Loss: 0.88113933801651\n",
            "Batch Num:411, Loss: 0.7681697607040405\n",
            "Batch Num:412, Loss: 0.8312538266181946\n",
            "Batch Num:413, Loss: 0.9949192404747009\n",
            "Batch Num:414, Loss: 0.7929399013519287\n",
            "Batch Num:415, Loss: 0.8652876019477844\n",
            "Batch Num:416, Loss: 0.8445001840591431\n",
            "Batch Num:417, Loss: 0.797144889831543\n",
            "Batch Num:418, Loss: 0.781125545501709\n",
            "Batch Num:419, Loss: 0.8316574096679688\n",
            "Batch Num:420, Loss: 0.8178681135177612\n",
            "Batch Num:421, Loss: 0.8016449213027954\n",
            "Batch Num:422, Loss: 0.7928271293640137\n",
            "Batch Num:423, Loss: 0.881328284740448\n",
            "Batch Num:424, Loss: 0.916462242603302\n",
            "Batch Num:425, Loss: 0.8030794262886047\n",
            "Batch Num:426, Loss: 0.8350522518157959\n",
            "Batch Num:427, Loss: 0.6937183737754822\n",
            "Batch Num:428, Loss: 0.6264676451683044\n",
            "Batch Num:429, Loss: 0.9206583499908447\n",
            "Batch Num:430, Loss: 0.8029332756996155\n",
            "Batch Num:431, Loss: 0.8649193644523621\n",
            "Batch Num:432, Loss: 0.7579404711723328\n",
            "Batch Num:433, Loss: 0.8831060528755188\n",
            "Batch Num:434, Loss: 0.8506114482879639\n",
            "Batch Num:435, Loss: 0.9666454195976257\n",
            "Batch Num:436, Loss: 0.8275507092475891\n",
            "Batch Num:437, Loss: 0.8999676704406738\n",
            "Batch Num:438, Loss: 0.8926488757133484\n",
            "Batch Num:439, Loss: 0.6166506409645081\n",
            "Batch Num:440, Loss: 0.7643369436264038\n",
            "Batch Num:441, Loss: 0.8489691615104675\n",
            "Batch Num:442, Loss: 0.6933833360671997\n",
            "Batch Num:443, Loss: 0.8156133890151978\n",
            "Batch Num:444, Loss: 1.0392330884933472\n",
            "Batch Num:445, Loss: 0.7406436204910278\n",
            "Batch Num:446, Loss: 0.854439377784729\n",
            "Batch Num:447, Loss: 0.7370532751083374\n",
            "Batch Num:448, Loss: 0.9395447373390198\n",
            "Batch Num:449, Loss: 0.7361970543861389\n",
            "Batch Num:450, Loss: 0.7124440670013428\n",
            "Batch Num:451, Loss: 0.9102665781974792\n",
            "Batch Num:452, Loss: 1.029363989830017\n",
            "Batch Num:453, Loss: 0.8122040033340454\n",
            "Batch Num:454, Loss: 0.6820266246795654\n",
            "Batch Num:455, Loss: 0.7056100964546204\n",
            "Batch Num:456, Loss: 0.7315753698348999\n",
            "Batch Num:457, Loss: 0.7589621543884277\n",
            "Batch Num:458, Loss: 0.5050835013389587\n",
            "Batch Num:459, Loss: 0.6925252676010132\n",
            "Batch Num:460, Loss: 0.8025826811790466\n",
            "Batch Num:461, Loss: 0.8680245876312256\n",
            "Batch Num:462, Loss: 0.6756957769393921\n",
            "Batch Num:463, Loss: 1.0153707265853882\n",
            "Batch Num:464, Loss: 0.915346622467041\n",
            "Batch Num:465, Loss: 0.826843798160553\n",
            "Batch Num:466, Loss: 0.9260738492012024\n",
            "Batch Num:467, Loss: 0.7050904631614685\n",
            "Batch Num:468, Loss: 0.7724119424819946\n",
            "Batch Num:469, Loss: 0.7982773780822754\n",
            "Batch Num:470, Loss: 0.6500337719917297\n",
            "Batch Num:471, Loss: 0.857728898525238\n",
            "Batch Num:472, Loss: 0.6799203157424927\n",
            "Batch Num:473, Loss: 0.6845473647117615\n",
            "Batch Num:474, Loss: 0.8157403469085693\n",
            "Batch Num:475, Loss: 0.9986261129379272\n",
            "Batch Num:476, Loss: 0.709017276763916\n",
            "Batch Num:477, Loss: 0.8300089240074158\n",
            "Batch Num:478, Loss: 0.710951566696167\n",
            "Batch Num:479, Loss: 0.9293259978294373\n",
            "Batch Num:480, Loss: 0.7381426095962524\n",
            "Batch Num:481, Loss: 0.825160801410675\n",
            "Batch Num:482, Loss: 0.6785852313041687\n",
            "Batch Num:483, Loss: 0.8039411306381226\n",
            "Batch Num:484, Loss: 0.6891102194786072\n",
            "Batch Num:485, Loss: 0.7071841955184937\n",
            "Batch Num:486, Loss: 0.726672887802124\n",
            "Batch Num:487, Loss: 0.6619582176208496\n",
            "Batch Num:488, Loss: 0.6716457009315491\n",
            "Batch Num:489, Loss: 0.8216601014137268\n",
            "Batch Num:490, Loss: 0.6027670502662659\n",
            "Batch Num:491, Loss: 0.7682536244392395\n",
            "Batch Num:492, Loss: 0.5616259574890137\n",
            "Batch Num:493, Loss: 0.7680515646934509\n",
            "Batch Num:494, Loss: 0.7309750318527222\n",
            "Batch Num:495, Loss: 0.7196654677391052\n",
            "Batch Num:496, Loss: 0.8217108845710754\n",
            "Batch Num:497, Loss: 0.638533890247345\n",
            "Batch Num:498, Loss: 0.7569482922554016\n",
            "Batch Num:499, Loss: 0.912864089012146\n",
            "Batch Num:500, Loss: 0.7630909085273743\n",
            "Batch Num:501, Loss: 0.6967260837554932\n",
            "Batch Num:502, Loss: 0.8025686740875244\n",
            "Batch Num:503, Loss: 0.7281136512756348\n",
            "Batch Num:504, Loss: 0.6624351143836975\n",
            "Batch Num:505, Loss: 0.7102811336517334\n",
            "Batch Num:506, Loss: 0.8844426870346069\n",
            "Batch Num:507, Loss: 0.649629533290863\n",
            "Batch Num:508, Loss: 0.9447356462478638\n",
            "Batch Num:509, Loss: 0.7727077007293701\n",
            "Batch Num:510, Loss: 0.7124234437942505\n",
            "Batch Num:511, Loss: 0.8909589052200317\n",
            "Batch Num:512, Loss: 0.718944787979126\n",
            "Batch Num:513, Loss: 0.7469752430915833\n",
            "Batch Num:514, Loss: 0.8481975793838501\n",
            "Batch Num:515, Loss: 0.8862132430076599\n",
            "Batch Num:516, Loss: 0.7309679985046387\n",
            "Batch Num:517, Loss: 0.7482437491416931\n",
            "Batch Num:518, Loss: 0.7538402080535889\n",
            "Batch Num:519, Loss: 0.8464040160179138\n",
            "Batch Num:520, Loss: 0.804534375667572\n",
            "Batch Num:521, Loss: 0.7899157404899597\n",
            "Batch Num:522, Loss: 0.6882627010345459\n",
            "Batch Num:523, Loss: 0.8227498531341553\n",
            "Batch Num:524, Loss: 0.5744246244430542\n",
            "Batch Num:525, Loss: 0.676489531993866\n",
            "Batch Num:526, Loss: 0.7733759880065918\n",
            "Batch Num:527, Loss: 0.7027943730354309\n",
            "Batch Num:528, Loss: 0.6235062479972839\n",
            "Batch Num:529, Loss: 0.6948902010917664\n",
            "Batch Num:530, Loss: 0.6463953256607056\n",
            "Batch Num:531, Loss: 0.670348048210144\n",
            "Batch Num:532, Loss: 0.8543186783790588\n",
            "Batch Num:533, Loss: 0.6025367975234985\n",
            "Batch Num:534, Loss: 0.6480562686920166\n",
            "Batch Num:535, Loss: 0.8311336636543274\n",
            "Batch Num:536, Loss: 0.7631911039352417\n",
            "Batch Num:537, Loss: 0.7019104361534119\n",
            "Batch Num:538, Loss: 0.6334489583969116\n",
            "Batch Num:539, Loss: 0.7253581285476685\n",
            "Batch Num:540, Loss: 0.7727627158164978\n",
            "Batch Num:541, Loss: 0.7677466869354248\n",
            "Batch Num:542, Loss: 0.7240954041481018\n",
            "Batch Num:543, Loss: 0.6323588490486145\n",
            "Batch Num:544, Loss: 0.642569899559021\n",
            "Batch Num:545, Loss: 0.7284311056137085\n",
            "Batch Num:546, Loss: 0.6933704018592834\n",
            "Batch Num:547, Loss: 0.711588978767395\n",
            "Batch Num:548, Loss: 0.7977623343467712\n",
            "Batch Num:549, Loss: 0.8951697945594788\n",
            "Batch Num:550, Loss: 0.7324137091636658\n",
            "Batch Num:551, Loss: 0.740885853767395\n",
            "Batch Num:552, Loss: 0.6816959977149963\n",
            "Batch Num:553, Loss: 0.7888883352279663\n",
            "Batch Num:554, Loss: 0.7909985184669495\n",
            "Batch Num:555, Loss: 0.6947782635688782\n",
            "Batch Num:556, Loss: 0.6581936478614807\n",
            "Batch Num:557, Loss: 0.6871351599693298\n",
            "Batch Num:558, Loss: 0.6917263865470886\n",
            "Batch Num:559, Loss: 0.76834636926651\n",
            "Batch Num:560, Loss: 0.8263850212097168\n",
            "Batch Num:561, Loss: 0.8625915050506592\n",
            "Batch Num:562, Loss: 0.7536149024963379\n",
            "Batch Num:563, Loss: 0.8228625059127808\n",
            "Batch Num:564, Loss: 0.845238983631134\n",
            "Batch Num:565, Loss: 0.6722614169120789\n",
            "Batch Num:566, Loss: 0.699468195438385\n",
            "Batch Num:567, Loss: 0.5575479865074158\n",
            "Batch Num:568, Loss: 0.648880660533905\n",
            "Batch Num:569, Loss: 0.6628902554512024\n",
            "Batch Num:570, Loss: 0.715347409248352\n",
            "Batch Num:571, Loss: 0.7293373942375183\n",
            "Batch Num:572, Loss: 0.6074509620666504\n",
            "Batch Num:573, Loss: 0.6286443471908569\n",
            "Batch Num:574, Loss: 0.7058133482933044\n",
            "Batch Num:575, Loss: 0.842349112033844\n",
            "Batch Num:576, Loss: 0.8548626899719238\n",
            "Batch Num:577, Loss: 0.7745095491409302\n",
            "Batch Num:578, Loss: 0.7538653016090393\n",
            "Batch Num:579, Loss: 0.7573029398918152\n",
            "Batch Num:580, Loss: 0.6756683588027954\n",
            "Batch Num:581, Loss: 0.8390788435935974\n",
            "Batch Num:582, Loss: 0.8353063464164734\n",
            "Batch Num:583, Loss: 0.7209544777870178\n",
            "Batch Num:584, Loss: 0.5637922883033752\n",
            "Batch Num:585, Loss: 0.9786086082458496\n",
            "Batch Num:586, Loss: 0.7759510278701782\n",
            "Batch Num:587, Loss: 0.8211392760276794\n",
            "Batch Num:588, Loss: 0.7601386308670044\n",
            "Batch Num:589, Loss: 0.493637353181839\n",
            "Batch Num:590, Loss: 0.7239441275596619\n",
            "Batch Num:591, Loss: 0.7131494283676147\n",
            "Batch Num:592, Loss: 0.5551050901412964\n",
            "Batch Num:593, Loss: 0.8185092806816101\n",
            "Batch Num:594, Loss: 0.787936806678772\n",
            "Batch Num:595, Loss: 0.5757874846458435\n",
            "Batch Num:596, Loss: 0.5757899284362793\n",
            "Batch Num:597, Loss: 0.7144644856452942\n",
            "Batch Num:598, Loss: 0.7753326296806335\n",
            "Batch Num:599, Loss: 0.8482392430305481\n",
            "Batch Num:600, Loss: 0.5530043244361877\n",
            "Batch Num:601, Loss: 0.7412942051887512\n",
            "Batch Num:602, Loss: 0.6946490406990051\n",
            "Batch Num:603, Loss: 0.6641498804092407\n",
            "Batch Num:604, Loss: 0.8878411054611206\n",
            "Batch Num:605, Loss: 0.667698323726654\n",
            "Batch Num:606, Loss: 0.5106296539306641\n",
            "Batch Num:607, Loss: 0.5619401931762695\n",
            "Batch Num:608, Loss: 0.8441969156265259\n",
            "Batch Num:609, Loss: 0.6429254412651062\n",
            "Batch Num:610, Loss: 0.6832432150840759\n",
            "Batch Num:611, Loss: 0.6514420509338379\n",
            "Batch Num:612, Loss: 0.4856491982936859\n",
            "Batch Num:613, Loss: 0.7505178451538086\n",
            "Batch Num:614, Loss: 0.6752431988716125\n",
            "Batch Num:615, Loss: 0.7284935116767883\n",
            "Batch Num:616, Loss: 0.9159019589424133\n",
            "Batch Num:617, Loss: 0.5884714126586914\n",
            "Batch Num:618, Loss: 0.6075938940048218\n",
            "Batch Num:619, Loss: 0.6672242879867554\n",
            "Batch Num:620, Loss: 0.6240004897117615\n",
            "Batch Num:621, Loss: 0.6794688701629639\n",
            "Batch Num:622, Loss: 0.6588038802146912\n",
            "Batch Num:623, Loss: 0.8498204946517944\n",
            "Batch Num:624, Loss: 0.6826983094215393\n",
            "Batch Num:625, Loss: 0.7279629707336426\n",
            "Batch Num:626, Loss: 0.7015091776847839\n",
            "Batch Num:627, Loss: 0.8914448618888855\n",
            "Batch Num:628, Loss: 0.5709909796714783\n",
            "Batch Num:629, Loss: 0.8700381517410278\n",
            "Batch Num:630, Loss: 0.8798227310180664\n",
            "Batch Num:631, Loss: 0.8560805320739746\n",
            "Batch Num:632, Loss: 0.707472562789917\n",
            "Batch Num:633, Loss: 0.7025086283683777\n",
            "Batch Num:634, Loss: 0.5990734100341797\n",
            "Batch Num:635, Loss: 0.6680813431739807\n",
            "Batch Num:636, Loss: 0.6103820204734802\n",
            "Batch Num:637, Loss: 0.5192089676856995\n",
            "Batch Num:638, Loss: 0.5967531800270081\n",
            "Batch Num:639, Loss: 0.7870076298713684\n",
            "Batch Num:640, Loss: 0.8076353073120117\n",
            "Batch Num:641, Loss: 0.6795139908790588\n",
            "Batch Num:642, Loss: 0.6154346466064453\n",
            "Batch Num:643, Loss: 0.6088440418243408\n",
            "Batch Num:644, Loss: 0.7840424180030823\n",
            "Batch Num:645, Loss: 0.6996870040893555\n",
            "Batch Num:646, Loss: 0.8666498064994812\n",
            "Batch Num:647, Loss: 0.6605151891708374\n",
            "Batch Num:648, Loss: 0.6317000389099121\n",
            "Batch Num:649, Loss: 0.5797443985939026\n",
            "Batch Num:650, Loss: 0.7730210423469543\n",
            "Batch Num:651, Loss: 0.6254346966743469\n",
            "Batch Num:652, Loss: 0.6406804323196411\n",
            "Batch Num:653, Loss: 0.6214316487312317\n",
            "Batch Num:654, Loss: 0.8043944835662842\n",
            "Batch Num:655, Loss: 0.576032280921936\n",
            "Batch Num:656, Loss: 0.8904308080673218\n",
            "Batch Num:657, Loss: 0.6244937777519226\n",
            "Batch Num:658, Loss: 0.6512867212295532\n",
            "Batch Num:659, Loss: 0.7975203990936279\n",
            "Batch Num:660, Loss: 0.80136638879776\n",
            "Batch Num:661, Loss: 0.6938848495483398\n",
            "Batch Num:662, Loss: 0.7061808109283447\n",
            "Batch Num:663, Loss: 0.8472615480422974\n",
            "Batch Num:664, Loss: 0.7706112265586853\n",
            "Batch Num:665, Loss: 0.5798915028572083\n",
            "Batch Num:666, Loss: 0.6429926753044128\n",
            "Batch Num:667, Loss: 0.5610570907592773\n",
            "Batch Num:668, Loss: 0.7274981737136841\n",
            "Batch Num:669, Loss: 0.8257745504379272\n",
            "Batch Num:670, Loss: 0.7079207897186279\n",
            "Batch Num:671, Loss: 0.7608948945999146\n",
            "Batch Num:672, Loss: 0.6875328421592712\n",
            "Batch Num:673, Loss: 0.8078995943069458\n",
            "Batch Num:674, Loss: 0.5496617555618286\n",
            "Batch Num:675, Loss: 0.569290816783905\n",
            "Batch Num:676, Loss: 0.7319360971450806\n",
            "Batch Num:677, Loss: 0.6610420942306519\n",
            "Batch Num:678, Loss: 0.606002688407898\n",
            "Batch Num:679, Loss: 0.7571683526039124\n",
            "Batch Num:680, Loss: 0.7821865677833557\n",
            "Batch Num:681, Loss: 0.8077972531318665\n",
            "Batch Num:682, Loss: 0.6410341858863831\n",
            "Batch Num:683, Loss: 0.6521768569946289\n",
            "Batch Num:684, Loss: 0.6756907105445862\n",
            "Batch Num:685, Loss: 0.6561315655708313\n",
            "Batch Num:686, Loss: 0.622403085231781\n",
            "Batch Num:687, Loss: 0.780072808265686\n",
            "Batch Num:688, Loss: 0.6411484479904175\n",
            "Batch Num:689, Loss: 0.6016892194747925\n",
            "Batch Num:690, Loss: 0.683422327041626\n",
            "Batch Num:691, Loss: 0.6915993094444275\n",
            "Batch Num:692, Loss: 0.7474496960639954\n",
            "Batch Num:693, Loss: 0.6436127424240112\n",
            "Batch Num:694, Loss: 0.7753562927246094\n",
            "Batch Num:695, Loss: 0.6228640675544739\n",
            "Batch Num:696, Loss: 0.7775707840919495\n",
            "Batch Num:697, Loss: 0.8922480940818787\n",
            "Batch Num:698, Loss: 0.7014552354812622\n",
            "Batch Num:699, Loss: 0.6990034580230713\n",
            "Batch Num:700, Loss: 0.5417808890342712\n",
            "Batch Num:701, Loss: 0.6083142161369324\n",
            "Batch Num:702, Loss: 0.561106264591217\n",
            "Batch Num:703, Loss: 0.7051206827163696\n",
            "Batch Num:704, Loss: 0.7838410139083862\n",
            "Batch Num:705, Loss: 0.703227698802948\n",
            "Batch Num:706, Loss: 0.7734262943267822\n",
            "Batch Num:707, Loss: 0.7992854714393616\n",
            "Batch Num:708, Loss: 0.645110011100769\n",
            "Batch Num:709, Loss: 0.5217524766921997\n",
            "Batch Num:710, Loss: 0.7310010194778442\n",
            "Batch Num:711, Loss: 0.6857200264930725\n",
            "Batch Num:712, Loss: 0.5682184100151062\n",
            "Batch Num:713, Loss: 0.7596708536148071\n",
            "Batch Num:714, Loss: 0.6375705003738403\n",
            "Batch Num:715, Loss: 0.5373444557189941\n",
            "Batch Num:716, Loss: 0.7729577422142029\n",
            "Batch Num:717, Loss: 0.6817507147789001\n",
            "Batch Num:718, Loss: 0.5913040637969971\n",
            "Batch Num:719, Loss: 0.6283167600631714\n",
            "Batch Num:720, Loss: 0.5855664610862732\n",
            "Batch Num:721, Loss: 0.5158355236053467\n",
            "Batch Num:722, Loss: 0.578429102897644\n",
            "Batch Num:723, Loss: 0.7685643434524536\n",
            "Batch Num:724, Loss: 0.5464702248573303\n",
            "Batch Num:725, Loss: 0.5220520496368408\n",
            "Batch Num:726, Loss: 0.7221255898475647\n",
            "Batch Num:727, Loss: 0.5738886594772339\n",
            "Batch Num:728, Loss: 0.7192570567131042\n",
            "Batch Num:729, Loss: 0.5601539611816406\n",
            "Batch Num:730, Loss: 0.8209533095359802\n",
            "Batch Num:731, Loss: 0.6453716158866882\n",
            "Batch Num:732, Loss: 0.695492148399353\n",
            "Batch Num:733, Loss: 0.5901261568069458\n",
            "Batch Num:734, Loss: 0.8845281004905701\n",
            "Batch Num:735, Loss: 0.5755364298820496\n",
            "Batch Num:736, Loss: 0.7365503311157227\n",
            "Batch Num:737, Loss: 0.6783444285392761\n",
            "Batch Num:738, Loss: 0.5873231887817383\n",
            "Batch Num:739, Loss: 0.6442808508872986\n",
            "Batch Num:740, Loss: 0.6038833260536194\n",
            "Batch Num:741, Loss: 0.7243247032165527\n",
            "Batch Num:742, Loss: 0.6688640117645264\n",
            "Batch Num:743, Loss: 0.5151492953300476\n",
            "Batch Num:744, Loss: 0.5529815554618835\n",
            "Batch Num:745, Loss: 0.551927924156189\n",
            "Batch Num:746, Loss: 0.5993021130561829\n",
            "Batch Num:747, Loss: 0.6569494009017944\n",
            "Batch Num:748, Loss: 0.7786308526992798\n",
            "Batch Num:749, Loss: 0.7211088538169861\n",
            "Batch Num:750, Loss: 0.8230111598968506\n",
            "Batch Num:751, Loss: 0.7234030961990356\n",
            "Batch Num:752, Loss: 0.7374319434165955\n",
            "Batch Num:753, Loss: 0.7479405403137207\n",
            "Batch Num:754, Loss: 0.5696337223052979\n",
            "Batch Num:755, Loss: 0.6930347681045532\n",
            "Batch Num:756, Loss: 0.5893465876579285\n",
            "Batch Num:757, Loss: 0.7851317524909973\n",
            "Batch Num:758, Loss: 0.743462860584259\n",
            "Batch Num:759, Loss: 0.5102761387825012\n",
            "Batch Num:760, Loss: 0.5709173679351807\n",
            "Batch Num:761, Loss: 0.6169021725654602\n",
            "Batch Num:762, Loss: 0.6129448413848877\n",
            "Batch Num:763, Loss: 0.791861355304718\n",
            "Batch Num:764, Loss: 0.5196141600608826\n",
            "Batch Num:765, Loss: 0.7498928904533386\n",
            "Batch Num:766, Loss: 0.6642195582389832\n",
            "Batch Num:767, Loss: 0.49366113543510437\n",
            "Batch Num:768, Loss: 0.8068445920944214\n",
            "Batch Num:769, Loss: 0.8168132901191711\n",
            "Batch Num:770, Loss: 0.8257981538772583\n",
            "Batch Num:771, Loss: 0.6554075479507446\n",
            "Batch Num:772, Loss: 0.7896862626075745\n",
            "Batch Num:773, Loss: 0.6930824518203735\n",
            "Batch Num:774, Loss: 0.8821495771408081\n",
            "Batch Num:775, Loss: 0.6766884326934814\n",
            "Batch Num:776, Loss: 0.7391096353530884\n",
            "Batch Num:777, Loss: 0.8051770925521851\n",
            "Batch Num:778, Loss: 0.5685008764266968\n",
            "Batch Num:779, Loss: 0.7344843149185181\n",
            "Batch Num:780, Loss: 0.5793027281761169\n",
            "Batch Num:781, Loss: 0.627431333065033\n",
            "Batch Num:782, Loss: 0.5735737681388855\n",
            "Batch Num:783, Loss: 0.7511330842971802\n",
            "Batch Num:784, Loss: 0.5832927823066711\n",
            "Batch Num:785, Loss: 0.7028172016143799\n",
            "Batch Num:786, Loss: 0.6134809255599976\n",
            "Batch Num:787, Loss: 0.7156378626823425\n",
            "Batch Num:788, Loss: 0.7578531503677368\n",
            "Batch Num:789, Loss: 0.560370147228241\n",
            "Batch Num:790, Loss: 0.5170438885688782\n",
            "Batch Num:791, Loss: 0.5373457670211792\n",
            "Batch Num:792, Loss: 0.6520823836326599\n",
            "Batch Num:793, Loss: 0.6902076601982117\n",
            "Batch Num:794, Loss: 0.8068300485610962\n",
            "Batch Num:795, Loss: 0.6557108759880066\n",
            "Batch Num:796, Loss: 0.6068830490112305\n",
            "Batch Num:797, Loss: 0.772246778011322\n",
            "Batch Num:798, Loss: 0.6256104707717896\n",
            "Batch Num:799, Loss: 0.8847548961639404\n",
            "Batch Num:800, Loss: 0.4736250638961792\n",
            "Batch Num:801, Loss: 0.5556502342224121\n",
            "Batch Num:802, Loss: 0.8184965252876282\n",
            "Batch Num:803, Loss: 0.7262576222419739\n",
            "Batch Num:804, Loss: 0.7396953105926514\n",
            "Batch Num:805, Loss: 0.7877722978591919\n",
            "Batch Num:806, Loss: 0.8586721420288086\n",
            "Batch Num:807, Loss: 0.5271338224411011\n",
            "Batch Num:808, Loss: 0.6553994417190552\n",
            "Batch Num:809, Loss: 0.6528303027153015\n",
            "Batch Num:810, Loss: 0.5448588132858276\n",
            "Batch Num:811, Loss: 0.7222935557365417\n",
            "Batch Num:812, Loss: 0.6799152493476868\n",
            "Batch Num:813, Loss: 0.6847376227378845\n",
            "Batch Num:814, Loss: 0.6917568445205688\n",
            "Batch Num:815, Loss: 0.8162162899971008\n",
            "Batch Num:816, Loss: 0.5342095494270325\n",
            "Batch Num:817, Loss: 0.8384205102920532\n",
            "Batch Num:818, Loss: 0.8520592451095581\n",
            "Batch Num:819, Loss: 0.678579568862915\n",
            "Batch Num:820, Loss: 0.6232613325119019\n",
            "Batch Num:821, Loss: 0.6667589545249939\n",
            "Batch Num:822, Loss: 0.6923291683197021\n",
            "Batch Num:823, Loss: 0.6279385685920715\n",
            "Batch Num:824, Loss: 0.7132230997085571\n",
            "Batch Num:825, Loss: 0.5471466183662415\n",
            "Batch Num:826, Loss: 0.8613272905349731\n",
            "Batch Num:827, Loss: 0.5713925957679749\n",
            "Batch Num:828, Loss: 0.5582960844039917\n",
            "Batch Num:829, Loss: 0.5900901556015015\n",
            "Batch Num:830, Loss: 0.6410195827484131\n",
            "Batch Num:831, Loss: 0.7135322093963623\n",
            "Batch Num:832, Loss: 0.7274871468544006\n",
            "Batch Num:833, Loss: 0.7111698389053345\n",
            "Batch Num:834, Loss: 0.6180930733680725\n",
            "Batch Num:835, Loss: 0.6050695776939392\n",
            "Batch Num:836, Loss: 0.48032936453819275\n",
            "Batch Num:837, Loss: 0.507485032081604\n",
            "Batch Num:838, Loss: 0.5924386978149414\n",
            "Batch Num:839, Loss: 0.6078110933303833\n",
            "Batch Num:840, Loss: 0.6823456883430481\n",
            "Batch Num:841, Loss: 0.6450344920158386\n",
            "Batch Num:842, Loss: 0.7638145685195923\n",
            "Batch Num:843, Loss: 0.3884877860546112\n",
            "Batch Num:844, Loss: 0.8235077261924744\n",
            "Batch Num:845, Loss: 0.7362706661224365\n",
            "Batch Num:846, Loss: 0.5621231198310852\n",
            "Batch Num:847, Loss: 0.5440495014190674\n",
            "Batch Num:848, Loss: 0.551365077495575\n",
            "Batch Num:849, Loss: 0.6541615724563599\n",
            "Batch Num:850, Loss: 0.5166134834289551\n",
            "Batch Num:851, Loss: 0.428488165140152\n",
            "Batch Num:852, Loss: 0.583656907081604\n",
            "Batch Num:853, Loss: 0.6126819849014282\n",
            "Batch Num:854, Loss: 0.8291776776313782\n",
            "Batch Num:855, Loss: 0.7425305843353271\n",
            "Batch Num:856, Loss: 0.6089636087417603\n",
            "Batch Num:857, Loss: 0.5017635822296143\n",
            "Batch Num:858, Loss: 0.5401360988616943\n",
            "Batch Num:859, Loss: 0.6851924657821655\n",
            "Batch Num:860, Loss: 0.5615628361701965\n",
            "Batch Num:861, Loss: 0.5974439382553101\n",
            "Batch Num:862, Loss: 0.6349262595176697\n",
            "Batch Num:863, Loss: 0.6485810875892639\n",
            "Batch Num:864, Loss: 0.8013454079627991\n",
            "Batch Num:865, Loss: 0.6214756369590759\n",
            "Batch Num:866, Loss: 0.5800325274467468\n",
            "Batch Num:867, Loss: 0.5958114862442017\n",
            "Batch Num:868, Loss: 0.6156319975852966\n",
            "Batch Num:869, Loss: 0.6103736758232117\n",
            "Batch Num:870, Loss: 0.4533833861351013\n",
            "Batch Num:871, Loss: 0.4980170726776123\n",
            "Batch Num:872, Loss: 0.545544445514679\n",
            "Batch Num:873, Loss: 0.6277608871459961\n",
            "Batch Num:874, Loss: 0.7150676846504211\n",
            "Batch Num:875, Loss: 0.7016699314117432\n",
            "Batch Num:876, Loss: 0.6155062317848206\n",
            "Batch Num:877, Loss: 0.6737642288208008\n",
            "Batch Num:878, Loss: 0.6567732095718384\n",
            "Batch Num:879, Loss: 0.47933682799339294\n",
            "Batch Num:880, Loss: 0.6340817809104919\n",
            "Batch Num:881, Loss: 0.6810721158981323\n",
            "Batch Num:882, Loss: 0.6089550852775574\n",
            "Batch Num:883, Loss: 0.7780939340591431\n",
            "Batch Num:884, Loss: 0.5369465947151184\n",
            "Batch Num:885, Loss: 0.7619571089744568\n",
            "Batch Num:886, Loss: 0.736850380897522\n",
            "Batch Num:887, Loss: 0.6696332693099976\n",
            "Batch Num:888, Loss: 0.5798807740211487\n",
            "Batch Num:889, Loss: 0.6243417263031006\n",
            "Batch Num:890, Loss: 0.5317925810813904\n",
            "Batch Num:891, Loss: 0.6085982322692871\n",
            "Batch Num:892, Loss: 0.3100127577781677\n",
            "Batch Num:893, Loss: 0.6245548725128174\n",
            "Batch Num:894, Loss: 0.616408109664917\n",
            "Batch Num:895, Loss: 0.6421103477478027\n",
            "Batch Num:896, Loss: 0.49138733744621277\n",
            "Batch Num:897, Loss: 0.551514208316803\n",
            "Batch Num:898, Loss: 0.6340658664703369\n",
            "Batch Num:899, Loss: 0.7303096652030945\n",
            "Batch Num:900, Loss: 0.5256714820861816\n",
            "Batch Num:901, Loss: 0.5840166211128235\n",
            "Batch Num:902, Loss: 0.6183778047561646\n",
            "Batch Num:903, Loss: 0.7109900712966919\n",
            "Batch Num:904, Loss: 0.7283036708831787\n",
            "Batch Num:905, Loss: 0.5285819172859192\n",
            "Batch Num:906, Loss: 0.5846194624900818\n",
            "Batch Num:907, Loss: 0.6291967630386353\n",
            "Batch Num:908, Loss: 0.6800063252449036\n",
            "Batch Num:909, Loss: 0.7186950445175171\n",
            "Batch Num:910, Loss: 0.6455749273300171\n",
            "Batch Num:911, Loss: 0.5345401763916016\n",
            "Batch Num:912, Loss: 0.5922219753265381\n",
            "Batch Num:913, Loss: 0.6160745024681091\n",
            "Batch Num:914, Loss: 0.5296468734741211\n",
            "Batch Num:915, Loss: 0.7079765796661377\n",
            "Batch Num:916, Loss: 0.5141040682792664\n",
            "Batch Num:917, Loss: 0.7219621539115906\n",
            "Batch Num:918, Loss: 0.474161297082901\n",
            "Batch Num:919, Loss: 0.47432273626327515\n",
            "Batch Num:920, Loss: 0.48167482018470764\n",
            "Batch Num:921, Loss: 0.5074000358581543\n",
            "Batch Num:922, Loss: 0.4496930241584778\n",
            "Batch Num:923, Loss: 0.5409159660339355\n",
            "Batch Num:924, Loss: 0.8017846941947937\n",
            "Batch Num:925, Loss: 0.5533215999603271\n",
            "Batch Num:926, Loss: 0.6625295877456665\n",
            "Batch Num:927, Loss: 0.6714102029800415\n",
            "Batch Num:928, Loss: 0.5059321522712708\n",
            "Batch Num:929, Loss: 0.4800952076911926\n",
            "Batch Num:930, Loss: 0.5144814848899841\n",
            "Batch Num:931, Loss: 0.5944843888282776\n",
            "Batch Num:932, Loss: 0.6778028607368469\n",
            "Batch Num:933, Loss: 0.5705801248550415\n",
            "Batch Num:934, Loss: 0.6475261449813843\n",
            "Batch Num:935, Loss: 0.5637803077697754\n",
            "Batch Num:936, Loss: 0.5563650727272034\n",
            "Batch Num:937, Loss: 0.5519092082977295\n",
            "Batch Num:938, Loss: 0.8960592150688171\n",
            "Training loss: 1.0100424232513412\n"
          ]
        }
      ]
    }
  ]
}